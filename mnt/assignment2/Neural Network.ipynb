{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset\n",
    "import gradient_check\n",
    "import layers\n",
    "import model\n",
    "import trainer\n",
    "import optim\n",
    "import metrics\n",
    "\n",
    "\n",
    "# from dataset import load_svhn, random_split_train_val\n",
    "# from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "# from layers import FullyConnectedLayer, ReLULayer\n",
    "# from model import TwoLayerNet\n",
    "# from trainer import Trainer, Dataset\n",
    "# from optim import SGD, MomentumSGD\n",
    "# from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = dataset.load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = dataset.random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3, 4],\n",
       "       [5, 6, 7]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[1,2,3], [4,5,6]])\n",
    "b = np.array([1,1,1])\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# importlib.reload(layers)\n",
    "# importlib.reload(gradient_check)\n",
    "\n",
    "# Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]])\n",
    "\n",
    "assert gradient_check.check_layer_gradient(layers.ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# importlib.reload(layers)\n",
    "# importlib.reload(gradient_check)\n",
    "\n",
    "# Implement FullyConnected layer forward and backward methods\n",
    "assert gradient_check.check_layer_gradient(layers.FullyConnectedLayer(3, 4), X)\n",
    "\n",
    "# Implement storing gradients for W and B\n",
    "assert gradient_check.check_layer_param_gradient(layers.FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert gradient_check.check_layer_param_gradient(layers.FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for 0_W\n",
      "Gradient check passed!\n",
      "Checking gradient for 0_B\n",
      "Gradient check passed!\n",
      "Checking gradient for 2_W\n",
      "Gradient check passed!\n",
      "Checking gradient for 2_B\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importlib.reload(model)\n",
    "# importlib.reload(layers)\n",
    "# importlib.reload(gradient_check)\n",
    "\n",
    "# In model.py, implement compute_loss_and_gradients function\n",
    "nn = model.TwoLayerNet(n_input=train_X.shape[1], n_output=10, hidden_layer_size=3, reg=0)\n",
    "loss = nn.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# Now implement backward pass and aggregate all of the params\n",
    "gradient_check.check_model_gradient(nn, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for 0_W\n",
      "Gradient check passed!\n",
      "Checking gradient for 0_B\n",
      "Gradient check passed!\n",
      "Checking gradient for 2_W\n",
      "Gradient check passed!\n",
      "Checking gradient for 2_B\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importlib.reload(model)\n",
    "# importlib.reload(layers)\n",
    "# importlib.reload(gradient_check)\n",
    "\n",
    "# Now implement L2 regularization in the forward and backward pass\n",
    "nn_with_reg = model.TwoLayerNet(n_input=train_X.shape[1], n_output =10, hidden_layer_size=3, reg=1e1)\n",
    "loss_with_reg = nn_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "gradient_check.check_model_gradient(nn_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importlib.reload(metrics)\n",
    "\n",
    "# Finally, implement predict function!\n",
    "# What would be the value we expect?\n",
    "metrics.multiclass_accuracy(nn_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.302397, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302413, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302501, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302320, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302424, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302483, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302622, Train accuracy: 0.148222, val accuracy: 0.140000\n",
      "Loss: 2.302511, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302463, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302548, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302555, Train accuracy: 0.148222, val accuracy: 0.140000\n",
      "Loss: 2.302588, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302482, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302539, Train accuracy: 0.093111, val accuracy: 0.090000\n",
      "Loss: 2.302533, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302574, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302560, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302537, Train accuracy: 0.114222, val accuracy: 0.147000\n",
      "Loss: 2.302554, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302400, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# importlib.reload(model)\n",
    "# importlib.reload(trainer)\n",
    "# importlib.reload(layers)\n",
    "# importlib.reload(gradient_check)\n",
    "\n",
    "nn = model.TwoLayerNet(n_input=train_X.shape[1], n_output=10, hidden_layer_size=100, reg=1e1)\n",
    "dataset = trainer.Dataset(train_X, train_y, val_X, val_y)\n",
    "coach = trainer.Trainer(nn, dataset, optim.SGD(), learning_rate=1e-2)\n",
    "\n",
    "# Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = coach.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.303586, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.293592, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.304204, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.251843, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.288600, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.278663, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.318597, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.228121, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.223722, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.276842, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.327759, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.287617, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.236164, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.203622, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.332110, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.266727, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.251408, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.266164, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.244875, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.357957, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# importlib.reload(trainer)\n",
    "\n",
    "# Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "nn = model.TwoLayerNet(n_input=train_X.shape[1], n_output=10, hidden_layer_size=100, reg=1e-1)\n",
    "dataset = trainer.Dataset(train_X, train_y, val_X, val_y)\n",
    "coach = trainer.Trainer(nn, dataset, optim.SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = coach.learning_rate\n",
    "loss_history, train_history, val_history = coach.fit()\n",
    "\n",
    "assert coach.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert coach.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn't've been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.327589, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.324036, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.317725, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.313724, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.316798, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.313802, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.304926, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.303641, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.299772, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.305861, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.303467, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.300830, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.304335, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.300111, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.300842, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.304221, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.296695, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.293623, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.289152, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.296703, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# importlib.reload(trainer)\n",
    "# importlib.reload(optim)\n",
    "\n",
    "# Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "nn = model.TwoLayerNet(n_input=train_X.shape[1], n_output=10, hidden_layer_size=100, reg=1e-1)\n",
    "dataset = trainer.Dataset(train_X, train_y, val_X, val_y)\n",
    "coach = trainer.Trainer(nn, dataset, optim.MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = coach.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.331183, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.330963, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.312516, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.323351, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.315293, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.283174, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.300711, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.303305, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.284461, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.104304, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.349776, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.212800, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.802621, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.106296, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.544965, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.879828, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.774081, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.040997, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.448184, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.056612, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.195587, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.852808, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.985908, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.684100, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.314774, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.154460, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 2.131799, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.603269, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.640418, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.297895, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.861744, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 2.008890, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 2.160913, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.355671, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.544487, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.261858, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.954305, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.427998, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.341209, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.849933, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.967224, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.795865, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.381115, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.667811, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.904801, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.356243, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 1.459221, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.930427, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.390823, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.320778, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.511497, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.737080, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.710631, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.372396, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 0.788517, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.205526, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.111951, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.121416, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.572132, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.642072, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.321048, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.555577, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 0.996476, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.322759, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.381675, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.759523, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.296026, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 0.824181, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.178981, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.603693, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.677949, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.469065, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.602791, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.646111, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.731389, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.390704, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.068337, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.561427, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.003431, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.328594, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.699840, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.643808, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.835070, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.738244, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.671432, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.367955, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.344110, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.246486, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.671045, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.222022, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.323465, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.196694, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.357610, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.211936, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.247717, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.212865, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.566927, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.167157, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.666619, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.560417, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.358503, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.362441, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 0.971887, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.845638, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.580342, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 0.978842, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.449553, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.156919, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.340950, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.267868, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.662038, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.417219, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.316503, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.012277, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.266817, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.259093, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.412088, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.059417, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.233548, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.425338, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.429269, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.407411, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.222271, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.432415, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.181028, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.459396, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.408506, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.046605, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.474543, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.167014, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.245833, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.394029, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.352508, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.414106, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.189782, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.266022, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.311468, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.244181, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.518584, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.321145, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.214566, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.484590, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.632272, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.413767, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.204817, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.124155, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.131232, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.138804, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.519260, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.092240, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "nn = model.TwoLayerNet(n_input=train_X.shape[1], n_output=10, hidden_layer_size=100, reg=1e-1)\n",
    "dataset = trainer.Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "coach = trainer.Trainer(nn, dataset, optim.SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = coach.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.307946, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.280654, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.249790, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.044726, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.794847, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.582240, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 1.734707, Train accuracy: 0.466667, val accuracy: 0.066667\n",
      "Loss: 1.491432, Train accuracy: 0.466667, val accuracy: 0.133333\n",
      "Loss: 1.662866, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.279243, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.204057, Train accuracy: 0.733333, val accuracy: 0.133333\n",
      "Loss: 0.430359, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 0.557082, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.082204, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.439251, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.075063, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 0.413806, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 0.292456, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.112483, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 0.008986, Train accuracy: 1.000000, val accuracy: 0.066667\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "nn = model.TwoLayerNet(n_input=train_X.shape[1], n_output=10, hidden_layer_size=100, reg=1e-4)\n",
    "dataset = trainer.Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "\n",
    "# Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "coach = trainer.Trainer(nn, dataset, optim.MomentumSGD(), learning_rate=1e-1, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = coach.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = 1e-4\n",
    "reg_strength = 1e-3\n",
    "learning_rate_decay = 0.999\n",
    "hidden_layer_size = 128\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
