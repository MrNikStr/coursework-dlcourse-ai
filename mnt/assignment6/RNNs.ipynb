{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
    "\n",
    "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P59NYU98GCb9"
   },
   "outputs": [],
   "source": [
    "# !pip3 -qq install torch==0.4.1\n",
    "# !pip3 -qq install bokeh==0.13.0\n",
    "# !pip3 -qq install gensim==3.6.0\n",
    "# !pip3 -qq install nltk\n",
    "# !pip3 -qq install scikit-learn==0.20.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8sVtGHmA9aBM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA :)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('Using CUDA :)')\n",
    "    from torch.cuda import FloatTensor, LongTensor\n",
    "else:\n",
    "    print('Using CPU :(')\n",
    "    from torch import FloatTensor, LongTensor\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6CNKM3b4hT1"
   },
   "source": [
    "# Рекуррентные нейронные сети (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O_XkoGNQUeGm"
   },
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QFEtWrS_4rUs"
   },
   "source": [
    "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
    "\n",
    "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
    "\n",
    "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
    "\n",
    "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
    "\n",
    "Мы порешаем сейчас POS Tagging для английского.\n",
    "\n",
    "Будем работать с таким набором тегов:\n",
    "- ADJ - adjective (new, good, high, ...)\n",
    "- ADP - adposition (on, of, at, ...)\n",
    "- ADV - adverb (really, already, still, ...)\n",
    "- CONJ - conjunction (and, or, but, ...)\n",
    "- DET - determiner, article (the, a, some, ...)\n",
    "- NOUN - noun (year, home, costs, ...)\n",
    "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
    "- PRT - particle (at, on, out, ...)\n",
    "- PRON - pronoun (he, their, her, ...)\n",
    "- VERB - verb (is, say, told, ...)\n",
    "- . - punctuation marks (. , ;)\n",
    "- X - other (ersatz, esprit, dunno, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPIkKdFlHB-X"
   },
   "source": [
    "Скачаем данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TiA2dGmgF1rW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/apollo/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/apollo/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d93g_swyJA_V"
   },
   "source": [
    "Пример размеченного предложения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QstS4NO0L97c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The            \tDET\n",
      "Fulton         \tNOUN\n",
      "County         \tNOUN\n",
      "Grand          \tADJ\n",
      "Jury           \tNOUN\n",
      "said           \tVERB\n",
      "Friday         \tNOUN\n",
      "an             \tDET\n",
      "investigation  \tNOUN\n",
      "of             \tADP\n",
      "Atlanta's      \tNOUN\n",
      "recent         \tADJ\n",
      "primary        \tNOUN\n",
      "election       \tNOUN\n",
      "produced       \tVERB\n",
      "``             \t.\n",
      "no             \tDET\n",
      "evidence       \tNOUN\n",
      "''             \t.\n",
      "that           \tADP\n",
      "any            \tDET\n",
      "irregularities \tNOUN\n",
      "took           \tVERB\n",
      "place          \tNOUN\n",
      ".              \t.\n"
     ]
    }
   ],
   "source": [
    "for word, tag in data[0]:\n",
    "    print('{:15}\\t{}'.format(word, tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "epdW8u_YXcAv"
   },
   "source": [
    "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
    "\n",
    "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTai8Ta0lgwL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences count in train/val/test: 36554/6451/14335\n",
      "Words count in train/val/test: 739769/130954/290469\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
    "\n",
    "wc_train = sum(len(sent) for sent in train_data)\n",
    "wc_val = sum(len(sent) for sent in val_data)\n",
    "wc_test = sum(len(sent) for sent in test_data)\n",
    "\n",
    "print(f'Sentences count in train/val/test: {len(train_data)}/{len(val_data)}/{len(test_data)}')\n",
    "print(f'Words count in train/val/test: {wc_train}/{wc_val}/{wc_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eChdLNGtXyP0"
   },
   "source": [
    "Построим маппинги из слов в индекс и из тега в индекс:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCjwwDs6Zq9x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in train = 45441. Tags = {'.', 'ADV', 'X', 'CONJ', 'NUM', 'PRT', 'VERB', 'ADP', 'PRON', 'ADJ', 'NOUN', 'DET'}\n"
     ]
    }
   ],
   "source": [
    "words = {word for sample in train_data for word, tag in sample}\n",
    "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
    "word2ind['<pad>'] = 0\n",
    "\n",
    "tags = {tag for sample in train_data for word, tag in sample}\n",
    "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
    "tag2ind['<pad>'] = 0\n",
    "\n",
    "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "URC1B2nvPGFt"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
    "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "bar_width = 0.35\n",
    "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
    "plt.xticks(np.arange(len(tags)), tags)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gArQwbzWWkgi"
   },
   "source": [
    "## Бейзлайн\n",
    "\n",
    "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
    "\n",
    "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
    "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
    "\n",
    "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
    "\n",
    "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
    "\n",
    "Простейший вариант - униграммная модель, учитывающая только слово:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5rWmSToIaeAo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of default tagger = 23.68%\n",
      "Accuracy of unigram tagger = 94.34%\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "default_tagger = nltk.DefaultTagger('NOUN') # Was: NN\n",
    "print('Accuracy of default tagger = {:.2%}'.format(default_tagger.evaluate(test_data)))\n",
    "\n",
    "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
    "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07Ymb_MkbWsF"
   },
   "source": [
    "Добавим вероятности переходов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vjz_Rk0bbMyH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of bigram tagger w/o backoff = 42.54%\n",
      "Accuracy of bigram tagger with backoff = 95.14%\n"
     ]
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(train_data)\n",
    "print('Accuracy of bigram tagger w/o backoff = {:.2%}'.format(bigram_tagger.evaluate(test_data)))\n",
    "\n",
    "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
    "print('Accuracy of bigram tagger with backoff = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWMw6QHvbaDd"
   },
   "source": [
    "Обратите внимание, что `backoff` важен:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8XCuxEBVbOY_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of trigram tagger w/o backoff = 23.33%\n",
      "Accuracy of trigram tagger with backoff = 95.15%\n"
     ]
    }
   ],
   "source": [
    "trigram_tagger = nltk.TrigramTagger(train_data)\n",
    "print('Accuracy of trigram tagger w/o backoff = {:.2%}'.format(trigram_tagger.evaluate(test_data)))\n",
    "\n",
    "trigram_tagger = nltk.TrigramTagger(train_data, backoff=bigram_tagger)\n",
    "print('Accuracy of trigram tagger with backoff = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4t3xyYd__8d-"
   },
   "source": [
    "## Увеличиваем контекст с рекуррентными сетями\n",
    "\n",
    "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
    "\n",
    "Омонимия - основная причина, почему униграмная модель плоха:  \n",
    "*“he cashed a check at the **bank**”*  \n",
    "vs  \n",
    "*“he sat on the **bank** of the river”*\n",
    "\n",
    "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
    "\n",
    "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
    "\n",
    "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
    "\n",
    "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtRbz1SwgEqc"
   },
   "outputs": [],
   "source": [
    "def convert_data(data, word2ind, tag2ind):\n",
    "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
    "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
    "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
    "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>29514</td>\n",
       "      <td>45335.0</td>\n",
       "      <td>26924.0</td>\n",
       "      <td>18946.0</td>\n",
       "      <td>22980.0</td>\n",
       "      <td>19998.0</td>\n",
       "      <td>27858.0</td>\n",
       "      <td>7556.0</td>\n",
       "      <td>11394.0</td>\n",
       "      <td>11720.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>17799</td>\n",
       "      <td>36693.0</td>\n",
       "      <td>12380.0</td>\n",
       "      <td>34255.0</td>\n",
       "      <td>26447.0</td>\n",
       "      <td>14038.0</td>\n",
       "      <td>27604.0</td>\n",
       "      <td>26447.0</td>\n",
       "      <td>36192.0</td>\n",
       "      <td>2899.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>41592</td>\n",
       "      <td>17922.0</td>\n",
       "      <td>13412.0</td>\n",
       "      <td>21518.0</td>\n",
       "      <td>16227.0</td>\n",
       "      <td>40385.0</td>\n",
       "      <td>25247.0</td>\n",
       "      <td>3520.0</td>\n",
       "      <td>43376.0</td>\n",
       "      <td>3513.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>31679</td>\n",
       "      <td>23661.0</td>\n",
       "      <td>28355.0</td>\n",
       "      <td>28328.0</td>\n",
       "      <td>43866.0</td>\n",
       "      <td>35347.0</td>\n",
       "      <td>26447.0</td>\n",
       "      <td>22659.0</td>\n",
       "      <td>23661.0</td>\n",
       "      <td>15078.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>29514</td>\n",
       "      <td>28247.0</td>\n",
       "      <td>12682.0</td>\n",
       "      <td>13947.0</td>\n",
       "      <td>44997.0</td>\n",
       "      <td>27930.0</td>\n",
       "      <td>16022.0</td>\n",
       "      <td>7365.0</td>\n",
       "      <td>5231.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 161 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0        1        2        3        4        5        6        7    \\\n",
       "0  29514  45335.0  26924.0  18946.0  22980.0  19998.0  27858.0   7556.0   \n",
       "1  17799  36693.0  12380.0  34255.0  26447.0  14038.0  27604.0  26447.0   \n",
       "2  41592  17922.0  13412.0  21518.0  16227.0  40385.0  25247.0   3520.0   \n",
       "3  31679  23661.0  28355.0  28328.0  43866.0  35347.0  26447.0  22659.0   \n",
       "4  29514  28247.0  12682.0  13947.0  44997.0  27930.0  16022.0   7365.0   \n",
       "\n",
       "       8        9    ...  151  152  153  154  155  156  157  158  159  160  \n",
       "0  11394.0  11720.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1  36192.0   2899.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "2  43376.0   3513.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "3  23661.0  15078.0  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "4   5231.0      NaN  ...  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[5 rows x 161 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_train).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 161 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2    3     4     5     6     7     8     9    ...  151  152  \\\n",
       "0    1  12.0   7.0  9.0   7.0   8.0  11.0  11.0   8.0  11.0  ...  NaN  NaN   \n",
       "1    8  12.0  11.0  8.0  12.0  10.0   8.0  12.0  10.0  11.0  ...  NaN  NaN   \n",
       "2   12  11.0   7.0  2.0   7.0   7.0   8.0  12.0   7.0  11.0  ...  NaN  NaN   \n",
       "3    2  10.0  11.0  2.0   7.0   8.0  12.0  10.0  10.0  11.0  ...  NaN  NaN   \n",
       "4    1   9.0   7.0  2.0   1.0   1.0   7.0  11.0   1.0   NaN  ...  NaN  NaN   \n",
       "\n",
       "   153  154  155  156  157  158  159  160  \n",
       "0  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "2  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "3  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "4  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
       "\n",
       "[5 rows x 161 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y_train).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhsTKZalfih6"
   },
   "outputs": [],
   "source": [
    "def iterate_batches(data, batch_size):\n",
    "    X, y = data\n",
    "    n_samples = len(X)\n",
    "\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        \n",
    "        batch_indices = indices[start:end]\n",
    "        \n",
    "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
    "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
    "        \n",
    "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
    "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
    "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
    "            \n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4XsRII5kW5x"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 4), (32, 4))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
    "\n",
    "X_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21593.,  2910.,  2591., 26447.],\n",
       "       [34418.,  6907., 27604.,   431.],\n",
       "       [34255., 27930., 44027., 22549.],\n",
       "       [26447., 28406., 25755., 29895.],\n",
       "       [ 1064., 14905., 34255., 27930.],\n",
       "       [27930., 18221., 29144., 36693.],\n",
       "       [26447., 37178., 27930.,  8575.],\n",
       "       [ 8091., 43150., 24031., 32316.],\n",
       "       [25247., 38031., 32866., 32545.],\n",
       "       [45172., 26447., 26447., 42279.],\n",
       "       [27930., 28091., 15289., 27930.],\n",
       "       [ 9663.,  9176., 11394., 32545.],\n",
       "       [15195., 27930., 26447., 31392.],\n",
       "       [ 5231., 12541., 43121.,   299.],\n",
       "       [    0., 31281., 21655., 10320.],\n",
       "       [    0., 42992., 27930., 22714.],\n",
       "       [    0., 10150., 26447., 27604.],\n",
       "       [    0., 11394., 10196., 16227.],\n",
       "       [    0., 26447., 11394., 15554.],\n",
       "       [    0., 32426., 32924.,  4432.],\n",
       "       [    0., 11394.,  9495., 34255.],\n",
       "       [    0., 19845.,  7787., 19769.],\n",
       "       [    0.,  5231., 38102.,  5231.],\n",
       "       [    0.,     0., 12279.,     0.],\n",
       "       [    0.,     0., 25597.,     0.],\n",
       "       [    0.,     0., 31688.,     0.],\n",
       "       [    0.,     0.,  9663.,     0.],\n",
       "       [    0.,     0., 10320.,     0.],\n",
       "       [    0.,     0.,  3520.,     0.],\n",
       "       [    0.,     0.,  8472.,     0.],\n",
       "       [    0.,     0., 34886.,     0.],\n",
       "       [    0.,     0.,  5231.,     0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3., 11.,  7., 12.],\n",
       "       [11., 11.,  8., 11.],\n",
       "       [ 8.,  1., 11.,  7.],\n",
       "       [12.,  9.,  2., 10.],\n",
       "       [11.,  7.,  8.,  1.],\n",
       "       [ 1., 11., 11., 12.],\n",
       "       [12.,  3.,  1., 11.],\n",
       "       [11.,  3., 11., 10.],\n",
       "       [ 8.,  8.,  7.,  4.],\n",
       "       [11., 12., 12.,  7.],\n",
       "       [ 1.,  7., 11.,  1.],\n",
       "       [ 9., 11.,  8.,  4.],\n",
       "       [ 7.,  1., 12.,  6.],\n",
       "       [ 1.,  7., 10.,  7.],\n",
       "       [ 0.,  5., 11.,  2.],\n",
       "       [ 0.,  2.,  1.,  7.],\n",
       "       [ 0., 10., 12.,  6.],\n",
       "       [ 0.,  8., 11.,  7.],\n",
       "       [ 0., 12.,  8., 10.],\n",
       "       [ 0., 11., 12., 11.],\n",
       "       [ 0.,  8., 12.,  8.],\n",
       "       [ 0., 11., 11.,  9.],\n",
       "       [ 0.,  1.,  7.,  1.],\n",
       "       [ 0.,  0.,  7.,  0.],\n",
       "       [ 0.,  0.,  7.,  0.],\n",
       "       [ 0.,  0.,  7.,  0.],\n",
       "       [ 0.,  0.,  9.,  0.],\n",
       "       [ 0.,  0.,  2.,  0.],\n",
       "       [ 0.,  0., 12.,  0.],\n",
       "       [ 0.,  0., 10.,  0.],\n",
       "       [ 0.,  0., 11.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5I9E9P6eFYv"
   },
   "source": [
    "**Задание** Реализуйте `LSTMTagger`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WVEHju54d68T"
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1, verbose=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=word_emb_dim)\n",
    "        self.lstm = nn.LSTM(input_size=word_emb_dim, hidden_size=lstm_hidden_dim, num_layers=lstm_layers_count)\n",
    "        self.linear = nn.Linear(in_features=lstm_hidden_dim, out_features=tagset_size)    \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if self.verbose:\n",
    "            print(f'Input shape: {inputs.shape}')\n",
    "            \n",
    "        embedding_res = self.embedding(inputs)\n",
    "        if self.verbose:\n",
    "            print(f'Embedding shape: {embedding_res.shape}')\n",
    "        \n",
    "        lstm_res, _ = self.lstm(embedding_res)\n",
    "        if self.verbose:\n",
    "            print(f'LSTM output shape: {lstm_res.shape}')\n",
    "        \n",
    "        linear_res = self.linear(lstm_res)\n",
    "        if self.verbose:\n",
    "            print(f'Prediction shape: {linear_res.shape}')\n",
    "        \n",
    "        return linear_res        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q_HA8zyheYGH"
   },
   "source": [
    "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbrxsZ2mehWB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 4])\n",
      "Embedding shape: torch.Size([32, 4, 100])\n",
      "LSTM output shape: torch.Size([32, 4, 128])\n",
      "Prediction shape: torch.Size([32, 4, 13])\n",
      "\n",
      "Prediction shape: torch.Size([32, 4, 13])\n",
      "True labels shape: torch.Size([32, 4])\n",
      "\n",
      "Accuracy: 0.0761\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "\n",
    "logits = model(X_batch)\n",
    "\n",
    "indices = torch.argmax(logits, -1)\n",
    "\n",
    "mask = y_batch.bool().float()\n",
    "match = (indices == y_batch).float()\n",
    "correct_samples = torch.sum(match * mask)\n",
    "total_samples = torch.sum(mask)\n",
    "accuracy = correct_samples/total_samples\n",
    "\n",
    "print(f'\\nPrediction shape: {logits.shape}')\n",
    "print(f'True labels shape: {y_batch.shape}')\n",
    "\n",
    "print(f'\\nAccuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GMUyUm1hgpe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.5737\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='mean').cuda()\n",
    "\n",
    "logits_4loss = torch.transpose(torch.transpose(logits, 0, 1), 1, 2)\n",
    "y_batch_4loss = torch.transpose(y_batch, 0, 1)\n",
    "\n",
    "loss = criterion(logits_4loss, y_batch_4loss)\n",
    "\n",
    "print(f'Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSgV3NPUpcjH"
   },
   "source": [
    "**Задание** Вставьте эти вычисление в функцию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FprPQ0gllo7b"
   },
   "outputs": [],
   "source": [
    "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
    "    epoch_loss = 0\n",
    "    correct_count = 0\n",
    "    sum_count = 0\n",
    "    \n",
    "    is_train = not optimizer is None\n",
    "    name = name or ''\n",
    "    model.train(is_train)\n",
    "    \n",
    "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
    "    \n",
    "    with torch.autograd.set_grad_enabled(is_train):\n",
    "        with tqdm(total=batches_count) as progress_bar:\n",
    "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
    "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "                logits = model(X_batch)\n",
    "\n",
    "                logits_4loss = torch.transpose(torch.transpose(logits, 0, 1), 1, 2)\n",
    "                y_batch_4loss = torch.transpose(y_batch, 0, 1)\n",
    "                loss = criterion(logits_4loss, y_batch_4loss)\n",
    "                indices = torch.argmax(logits, -1)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if optimizer:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                                \n",
    "                mask = y_batch.bool().float()\n",
    "                match = (indices == y_batch).float()\n",
    "                cur_correct_count = torch.sum(match * mask)\n",
    "                cur_sum_count = torch.sum(mask)\n",
    "\n",
    "                correct_count += cur_correct_count\n",
    "                sum_count += cur_sum_count\n",
    "\n",
    "                progress_bar.update()\n",
    "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
    "                )\n",
    "                \n",
    "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
    "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
    "            )\n",
    "\n",
    "    return epoch_loss / batches_count, correct_count / sum_count\n",
    "\n",
    "\n",
    "def fit(model, criterion, optimizer, train_data, \n",
    "        epochs_count=1, batch_size=32, \n",
    "        val_data=None, val_batch_size=None, \n",
    "        scheduler=None, scheduler_loss=False):\n",
    "        \n",
    "    if not val_data is None and val_batch_size is None:\n",
    "        val_batch_size = batch_size\n",
    "        \n",
    "    for epoch in range(epochs_count):\n",
    "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
    "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
    "        \n",
    "        if not val_data is None:\n",
    "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')\n",
    "            \n",
    "        if scheduler is not None:\n",
    "            if scheduler_loss:\n",
    "                scheduler.step(val_loss)\n",
    "            else:\n",
    "                scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 0.77425, Accuracy = 75.08%: 100%|██████████| 572/572 [00:04<00:00, 119.81it/s]\n",
      "[1 / 50]   Val: Loss = 0.49991, Accuracy = 83.28%: 100%|██████████| 13/13 [00:00<00:00, 150.32it/s]\n",
      "[2 / 50] Train: Loss = 0.42691, Accuracy = 85.62%: 100%|██████████| 572/572 [00:04<00:00, 116.82it/s]\n",
      "[2 / 50]   Val: Loss = 0.38644, Accuracy = 86.90%: 100%|██████████| 13/13 [00:00<00:00, 132.25it/s]\n",
      "[3 / 50] Train: Loss = 0.33299, Accuracy = 88.85%: 100%|██████████| 572/572 [00:04<00:00, 121.52it/s]\n",
      "[3 / 50]   Val: Loss = 0.30540, Accuracy = 89.33%: 100%|██████████| 13/13 [00:00<00:00, 144.74it/s]\n",
      "[4 / 50] Train: Loss = 0.25188, Accuracy = 91.69%: 100%|██████████| 572/572 [00:04<00:00, 116.66it/s]\n",
      "[4 / 50]   Val: Loss = 0.24288, Accuracy = 91.55%: 100%|██████████| 13/13 [00:00<00:00, 147.33it/s]\n",
      "[5 / 50] Train: Loss = 0.18765, Accuracy = 93.90%: 100%|██████████| 572/572 [00:04<00:00, 119.17it/s]\n",
      "[5 / 50]   Val: Loss = 0.19640, Accuracy = 93.12%: 100%|██████████| 13/13 [00:00<00:00, 137.00it/s]\n",
      "[6 / 50] Train: Loss = 0.14849, Accuracy = 95.13%: 100%|██████████| 572/572 [00:04<00:00, 118.25it/s]\n",
      "[6 / 50]   Val: Loss = 0.16951, Accuracy = 93.92%: 100%|██████████| 13/13 [00:00<00:00, 141.61it/s]\n",
      "[7 / 50] Train: Loss = 0.12509, Accuracy = 95.85%: 100%|██████████| 572/572 [00:05<00:00, 113.81it/s]\n",
      "[7 / 50]   Val: Loss = 0.14857, Accuracy = 94.68%: 100%|██████████| 13/13 [00:00<00:00, 136.25it/s]\n",
      "[8 / 50] Train: Loss = 0.11349, Accuracy = 96.19%: 100%|██████████| 572/572 [00:04<00:00, 125.10it/s]\n",
      "[8 / 50]   Val: Loss = 0.13579, Accuracy = 95.41%: 100%|██████████| 13/13 [00:00<00:00, 129.91it/s]\n",
      "[9 / 50] Train: Loss = 0.10678, Accuracy = 96.42%: 100%|██████████| 572/572 [00:04<00:00, 131.47it/s]\n",
      "[9 / 50]   Val: Loss = 0.13779, Accuracy = 95.28%: 100%|██████████| 13/13 [00:00<00:00, 140.17it/s]\n",
      "[10 / 50] Train: Loss = 0.10213, Accuracy = 96.57%: 100%|██████████| 572/572 [00:04<00:00, 123.24it/s]\n",
      "[10 / 50]   Val: Loss = 0.13342, Accuracy = 95.57%: 100%|██████████| 13/13 [00:00<00:00, 142.62it/s]\n",
      "[11 / 50] Train: Loss = 0.09902, Accuracy = 96.68%: 100%|██████████| 572/572 [00:04<00:00, 138.98it/s]\n",
      "[11 / 50]   Val: Loss = 0.13302, Accuracy = 95.53%: 100%|██████████| 13/13 [00:00<00:00, 136.70it/s]\n",
      "[12 / 50] Train: Loss = 0.09601, Accuracy = 96.78%: 100%|██████████| 572/572 [00:03<00:00, 150.27it/s]\n",
      "[12 / 50]   Val: Loss = 0.13028, Accuracy = 95.64%: 100%|██████████| 13/13 [00:00<00:00, 142.77it/s]\n",
      "[13 / 50] Train: Loss = 0.09401, Accuracy = 96.87%: 100%|██████████| 572/572 [00:04<00:00, 129.68it/s]\n",
      "[13 / 50]   Val: Loss = 0.13143, Accuracy = 95.65%: 100%|██████████| 13/13 [00:00<00:00, 142.88it/s]\n",
      "[14 / 50] Train: Loss = 0.09206, Accuracy = 96.93%: 100%|██████████| 572/572 [00:04<00:00, 120.37it/s]\n",
      "[14 / 50]   Val: Loss = 0.13083, Accuracy = 95.62%: 100%|██████████| 13/13 [00:00<00:00, 151.82it/s]\n",
      "[15 / 50] Train: Loss = 0.09095, Accuracy = 96.98%: 100%|██████████| 572/572 [00:04<00:00, 118.31it/s]\n",
      "[15 / 50]   Val: Loss = 0.12871, Accuracy = 95.66%: 100%|██████████| 13/13 [00:00<00:00, 144.39it/s]\n",
      "[16 / 50] Train: Loss = 0.08923, Accuracy = 97.05%: 100%|██████████| 572/572 [00:04<00:00, 128.85it/s]\n",
      "[16 / 50]   Val: Loss = 0.12893, Accuracy = 95.65%: 100%|██████████| 13/13 [00:00<00:00, 151.94it/s]\n",
      "[17 / 50] Train: Loss = 0.08808, Accuracy = 97.09%: 100%|██████████| 572/572 [00:04<00:00, 131.58it/s]\n",
      "[17 / 50]   Val: Loss = 0.12741, Accuracy = 95.74%: 100%|██████████| 13/13 [00:00<00:00, 141.45it/s]\n",
      "[18 / 50] Train: Loss = 0.08668, Accuracy = 97.14%: 100%|██████████| 572/572 [00:05<00:00, 114.03it/s]\n",
      "[18 / 50]   Val: Loss = 0.12551, Accuracy = 95.83%: 100%|██████████| 13/13 [00:00<00:00, 148.88it/s]\n",
      "[19 / 50] Train: Loss = 0.08523, Accuracy = 97.20%: 100%|██████████| 572/572 [00:04<00:00, 114.96it/s]\n",
      "[19 / 50]   Val: Loss = 0.12788, Accuracy = 95.77%: 100%|██████████| 13/13 [00:00<00:00, 139.36it/s]\n",
      "[20 / 50] Train: Loss = 0.08452, Accuracy = 97.23%: 100%|██████████| 572/572 [00:04<00:00, 124.30it/s]\n",
      "[20 / 50]   Val: Loss = 0.12831, Accuracy = 95.71%: 100%|██████████| 13/13 [00:00<00:00, 143.62it/s]\n",
      "[21 / 50] Train: Loss = 0.08377, Accuracy = 97.27%: 100%|██████████| 572/572 [00:04<00:00, 118.96it/s]\n",
      "[21 / 50]   Val: Loss = 0.12853, Accuracy = 95.73%: 100%|██████████| 13/13 [00:00<00:00, 145.74it/s]\n",
      "[22 / 50] Train: Loss = 0.08215, Accuracy = 97.33%: 100%|██████████| 572/572 [00:04<00:00, 131.47it/s]\n",
      "[22 / 50]   Val: Loss = 0.12654, Accuracy = 95.71%: 100%|██████████| 13/13 [00:00<00:00, 148.10it/s]\n",
      "[23 / 50] Train: Loss = 0.05900, Accuracy = 98.20%:   6%|▌         | 32/572 [00:00<00:03, 165.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    21: reducing learning rate of group 0 to 3.3300e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23 / 50] Train: Loss = 0.06919, Accuracy = 97.80%: 100%|██████████| 572/572 [00:04<00:00, 137.71it/s]\n",
      "[23 / 50]   Val: Loss = 0.12040, Accuracy = 95.97%: 100%|██████████| 13/13 [00:00<00:00, 136.12it/s]\n",
      "[24 / 50] Train: Loss = 0.06678, Accuracy = 97.92%: 100%|██████████| 572/572 [00:04<00:00, 138.95it/s]\n",
      "[24 / 50]   Val: Loss = 0.12319, Accuracy = 95.94%: 100%|██████████| 13/13 [00:00<00:00, 152.09it/s]\n",
      "[25 / 50] Train: Loss = 0.06696, Accuracy = 97.93%: 100%|██████████| 572/572 [00:04<00:00, 140.67it/s]\n",
      "[25 / 50]   Val: Loss = 0.12106, Accuracy = 95.97%: 100%|██████████| 13/13 [00:00<00:00, 129.77it/s]\n",
      "[26 / 50] Train: Loss = 0.06667, Accuracy = 97.93%: 100%|██████████| 572/572 [00:03<00:00, 146.08it/s]\n",
      "[26 / 50]   Val: Loss = 0.12177, Accuracy = 95.92%: 100%|██████████| 13/13 [00:00<00:00, 139.64it/s]\n",
      "[27 / 50] Train: Loss = 0.06607, Accuracy = 97.94%: 100%|██████████| 572/572 [00:04<00:00, 136.06it/s] \n",
      "[27 / 50]   Val: Loss = 0.12382, Accuracy = 95.96%: 100%|██████████| 13/13 [00:00<00:00, 139.19it/s]\n",
      "[28 / 50] Train: Loss = 0.05039, Accuracy = 98.49%:   5%|▍         | 27/572 [00:00<00:04, 134.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    26: reducing learning rate of group 0 to 1.1089e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[28 / 50] Train: Loss = 0.05974, Accuracy = 98.18%: 100%|██████████| 572/572 [00:04<00:00, 128.66it/s]\n",
      "[28 / 50]   Val: Loss = 0.12221, Accuracy = 95.99%: 100%|██████████| 13/13 [00:00<00:00, 148.91it/s]\n",
      "[29 / 50] Train: Loss = 0.05929, Accuracy = 98.21%: 100%|██████████| 572/572 [00:04<00:00, 122.79it/s]\n",
      "[29 / 50]   Val: Loss = 0.12130, Accuracy = 95.97%: 100%|██████████| 13/13 [00:00<00:00, 151.90it/s]\n",
      "[30 / 50] Train: Loss = 0.05915, Accuracy = 98.24%: 100%|██████████| 572/572 [00:04<00:00, 130.99it/s]\n",
      "[30 / 50]   Val: Loss = 0.12204, Accuracy = 95.94%: 100%|██████████| 13/13 [00:00<00:00, 130.73it/s]\n",
      "[31 / 50] Train: Loss = 0.05899, Accuracy = 98.25%: 100%|██████████| 572/572 [00:04<00:00, 120.80it/s]\n",
      "[31 / 50]   Val: Loss = 0.12328, Accuracy = 95.94%: 100%|██████████| 13/13 [00:00<00:00, 142.74it/s]\n",
      "[32 / 50] Train: Loss = 0.06469, Accuracy = 98.49%:   5%|▍         | 27/572 [00:00<00:03, 139.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    30: reducing learning rate of group 0 to 3.6926e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[32 / 50] Train: Loss = 0.05623, Accuracy = 98.34%: 100%|██████████| 572/572 [00:04<00:00, 123.51it/s]\n",
      "[32 / 50]   Val: Loss = 0.12414, Accuracy = 95.94%: 100%|██████████| 13/13 [00:00<00:00, 139.97it/s]\n",
      "[33 / 50] Train: Loss = 0.05608, Accuracy = 98.36%: 100%|██████████| 572/572 [00:04<00:00, 128.21it/s]\n",
      "[33 / 50]   Val: Loss = 0.12312, Accuracy = 95.94%: 100%|██████████| 13/13 [00:00<00:00, 138.48it/s]\n",
      "[34 / 50] Train: Loss = 0.05593, Accuracy = 98.37%: 100%|██████████| 572/572 [00:04<00:00, 142.62it/s]\n",
      "[34 / 50]   Val: Loss = 0.12460, Accuracy = 95.93%: 100%|██████████| 13/13 [00:00<00:00, 144.77it/s]\n",
      "[35 / 50] Train: Loss = 0.05593, Accuracy = 98.37%: 100%|██████████| 572/572 [00:04<00:00, 125.94it/s]\n",
      "[35 / 50]   Val: Loss = 0.12512, Accuracy = 95.90%: 100%|██████████| 13/13 [00:00<00:00, 138.17it/s]\n",
      "[36 / 50] Train: Loss = 0.05882, Accuracy = 98.07%:   5%|▍         | 27/572 [00:00<00:04, 132.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    34: reducing learning rate of group 0 to 1.2296e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[36 / 50] Train: Loss = 0.05478, Accuracy = 98.42%: 100%|██████████| 572/572 [00:04<00:00, 119.43it/s] \n",
      "[36 / 50]   Val: Loss = 0.12463, Accuracy = 95.91%: 100%|██████████| 13/13 [00:00<00:00, 149.82it/s]\n",
      "[37 / 50] Train: Loss = 0.05474, Accuracy = 98.42%: 100%|██████████| 572/572 [00:04<00:00, 122.04it/s]\n",
      "[37 / 50]   Val: Loss = 0.12512, Accuracy = 95.91%: 100%|██████████| 13/13 [00:00<00:00, 142.38it/s]\n",
      "[38 / 50] Train: Loss = 0.05473, Accuracy = 98.41%: 100%|██████████| 572/572 [00:04<00:00, 119.37it/s]\n",
      "[38 / 50]   Val: Loss = 0.12476, Accuracy = 95.91%: 100%|██████████| 13/13 [00:00<00:00, 138.88it/s]\n",
      "[39 / 50] Train: Loss = 0.05465, Accuracy = 98.43%: 100%|██████████| 572/572 [00:04<00:00, 117.10it/s]\n",
      "[39 / 50]   Val: Loss = 0.12548, Accuracy = 95.90%: 100%|██████████| 13/13 [00:00<00:00, 152.35it/s]\n",
      "[40 / 50] Train: Loss = 0.04704, Accuracy = 98.56%:   5%|▍         | 27/572 [00:00<00:03, 138.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    38: reducing learning rate of group 0 to 4.0947e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[40 / 50] Train: Loss = 0.05431, Accuracy = 98.44%: 100%|██████████| 572/572 [00:04<00:00, 121.03it/s]\n",
      "[40 / 50]   Val: Loss = 0.12524, Accuracy = 95.90%: 100%|██████████| 13/13 [00:00<00:00, 141.86it/s]\n",
      "[41 / 50] Train: Loss = 0.05426, Accuracy = 98.43%: 100%|██████████| 572/572 [00:04<00:00, 122.42it/s]\n",
      "[41 / 50]   Val: Loss = 0.12490, Accuracy = 95.91%: 100%|██████████| 13/13 [00:00<00:00, 138.81it/s]\n",
      "[42 / 50] Train: Loss = 0.05431, Accuracy = 98.44%: 100%|██████████| 572/572 [00:04<00:00, 117.38it/s]\n",
      "[42 / 50]   Val: Loss = 0.12550, Accuracy = 95.90%: 100%|██████████| 13/13 [00:00<00:00, 149.46it/s]\n",
      "[43 / 50] Train: Loss = 0.05423, Accuracy = 98.44%: 100%|██████████| 572/572 [00:05<00:00, 108.76it/s]\n",
      "[43 / 50]   Val: Loss = 0.12511, Accuracy = 95.89%: 100%|██████████| 13/13 [00:00<00:00, 148.11it/s]\n",
      "[44 / 50] Train: Loss = 0.05782, Accuracy = 98.13%:   4%|▍         | 23/572 [00:00<00:04, 124.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    42: reducing learning rate of group 0 to 1.3635e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[44 / 50] Train: Loss = 0.05409, Accuracy = 98.44%: 100%|██████████| 572/572 [00:04<00:00, 123.93it/s]\n",
      "[44 / 50]   Val: Loss = 0.12560, Accuracy = 95.90%: 100%|██████████| 13/13 [00:00<00:00, 143.18it/s]\n",
      "[45 / 50] Train: Loss = 0.05416, Accuracy = 98.44%: 100%|██████████| 572/572 [00:04<00:00, 118.64it/s]\n",
      "[45 / 50]   Val: Loss = 0.12509, Accuracy = 95.89%: 100%|██████████| 13/13 [00:00<00:00, 147.52it/s]\n",
      "[46 / 50] Train: Loss = 0.05408, Accuracy = 98.44%: 100%|██████████| 572/572 [00:04<00:00, 115.59it/s]\n",
      "[46 / 50]   Val: Loss = 0.12500, Accuracy = 95.90%: 100%|██████████| 13/13 [00:00<00:00, 142.70it/s]\n",
      "[47 / 50] Train: Loss = 0.05412, Accuracy = 98.44%: 100%|██████████| 572/572 [00:04<00:00, 118.74it/s]\n",
      "[47 / 50]   Val: Loss = 0.12528, Accuracy = 95.89%: 100%|██████████| 13/13 [00:00<00:00, 150.25it/s]\n",
      "[48 / 50] Train: Loss = 0.04157, Accuracy = 98.80%:   4%|▍         | 24/572 [00:00<00:04, 128.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    46: reducing learning rate of group 0 to 4.5406e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[48 / 50] Train: Loss = 0.05414, Accuracy = 98.45%: 100%|██████████| 572/572 [00:05<00:00, 114.39it/s]\n",
      "[48 / 50]   Val: Loss = 0.12502, Accuracy = 95.90%: 100%|██████████| 13/13 [00:00<00:00, 129.23it/s]\n",
      "[49 / 50] Train: Loss = 0.05402, Accuracy = 98.44%: 100%|██████████| 572/572 [00:04<00:00, 115.20it/s] \n",
      "[49 / 50]   Val: Loss = 0.12505, Accuracy = 95.90%: 100%|██████████| 13/13 [00:00<00:00, 143.39it/s]\n",
      "[50 / 50] Train: Loss = 0.05401, Accuracy = 98.44%: 100%|██████████| 572/572 [00:04<00:00, 116.30it/s]\n",
      "[50 / 50]   Val: Loss = 0.12540, Accuracy = 95.90%: 100%|██████████| 13/13 [00:00<00:00, 143.44it/s]\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1.5e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.333, patience=3, verbose=True)\n",
    "\n",
    "fit(model, criterion, optimizer, train_data=(X_train, y_train), \n",
    "    epochs_count=50, batch_size=64, \n",
    "    val_data=(X_val, y_val), val_batch_size=512,\n",
    "    scheduler=scheduler, scheduler_loss=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0qGetIhfUE5"
   },
   "source": [
    "### Masking\n",
    "\n",
    "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
    "\n",
    "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAfV2dEOfHo5"
   },
   "source": [
    "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98wr38_rw55D"
   },
   "outputs": [],
   "source": [
    "def compute_loss_accuracy(model, criterion, data, batch_size=512):    \n",
    "    model.eval() # Evaluation mode\n",
    "    \n",
    "    with torch.no_grad():    \n",
    "        loss_accum = 0.0\n",
    "        correct_count = 0.0\n",
    "        sum_count = 0.0\n",
    "        for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
    "            X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
    "            logits = model(X_batch)       \n",
    "            \n",
    "            logits_4loss = torch.transpose(torch.transpose(logits, 0, 1), 1, 2)\n",
    "            y_batch_4loss = torch.transpose(y_batch, 0, 1)\n",
    "            loss = criterion(logits_4loss, y_batch_4loss)\n",
    "            indices = torch.argmax(logits, -1)\n",
    "            \n",
    "            loss_accum += loss.item()\n",
    "            \n",
    "            mask = y_batch.bool().float()\n",
    "            match = (indices == y_batch).float()\n",
    "            cur_correct_count = torch.sum(match * mask)\n",
    "            cur_sum_count = torch.sum(mask)\n",
    "\n",
    "            correct_count += cur_correct_count\n",
    "            sum_count += cur_sum_count\n",
    "    \n",
    "    return loss_accum / (i + 1), correct_count / sum_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss = 0.12384, Accuracy = 96.00%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = compute_loss_accuracy(model, criterion, data=(X_test, y_test))\n",
    "print('Test: Loss = {:.5f}, Accuracy = {:.2%}'.format(test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PXUTSFaEHbDG"
   },
   "source": [
    "### Bidirectional LSTM\n",
    "\n",
    "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
    "\n",
    "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
    "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
    "\n",
    "**Задание** Добавьте Bidirectional LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalLSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1, verbose=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=word_emb_dim)\n",
    "        self.lstm = nn.LSTM(input_size=word_emb_dim, hidden_size=lstm_hidden_dim, num_layers=lstm_layers_count, \n",
    "                            bidirectional=True)\n",
    "        self.linear = nn.Linear(in_features=2*lstm_hidden_dim, out_features=tagset_size)    \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if self.verbose:\n",
    "            print(f'Input shape: {inputs.shape}')\n",
    "            \n",
    "        embedding_res = self.embedding(inputs)\n",
    "        if self.verbose:\n",
    "            print(f'Embedding shape: {embedding_res.shape}')\n",
    "        \n",
    "        lstm_res, _ = self.lstm(embedding_res)\n",
    "        if self.verbose:\n",
    "            print(f'LSTM output shape: {lstm_res.shape}')\n",
    "        \n",
    "        linear_res = self.linear(lstm_res)\n",
    "        if self.verbose:\n",
    "            print(f'Prediction shape: {linear_res.shape}')\n",
    "        \n",
    "        return linear_res  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 4])\n",
      "Embedding shape: torch.Size([32, 4, 100])\n",
      "LSTM output shape: torch.Size([32, 4, 256])\n",
      "Prediction shape: torch.Size([32, 4, 13])\n",
      "\n",
      "Prediction shape: torch.Size([32, 4, 13])\n",
      "True labels shape: torch.Size([32, 4])\n",
      "\n",
      "Accuracy: 0.1304\n"
     ]
    }
   ],
   "source": [
    "model = BidirectionalLSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "\n",
    "logits = model(X_batch)\n",
    "\n",
    "indices = torch.argmax(logits, -1)\n",
    "\n",
    "mask = y_batch.bool().float()\n",
    "match = (indices == y_batch).float()\n",
    "correct_samples = torch.sum(match * mask)\n",
    "total_samples = torch.sum(mask)\n",
    "accuracy = correct_samples/total_samples\n",
    "\n",
    "print(f'\\nPrediction shape: {logits.shape}')\n",
    "print(f'True labels shape: {y_batch.shape}')\n",
    "\n",
    "print(f'\\nAccuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 0.62997, Accuracy = 79.89%: 100%|██████████| 572/572 [00:06<00:00, 82.30it/s]\n",
      "[1 / 50]   Val: Loss = 0.38582, Accuracy = 87.06%: 100%|██████████| 13/13 [00:00<00:00, 113.57it/s]\n",
      "[2 / 50] Train: Loss = 0.33059, Accuracy = 89.10%: 100%|██████████| 572/572 [00:06<00:00, 83.02it/s]\n",
      "[2 / 50]   Val: Loss = 0.30252, Accuracy = 89.81%: 100%|██████████| 13/13 [00:00<00:00, 111.72it/s]\n",
      "[3 / 50] Train: Loss = 0.26269, Accuracy = 91.44%: 100%|██████████| 572/572 [00:07<00:00, 79.98it/s]\n",
      "[3 / 50]   Val: Loss = 0.23863, Accuracy = 92.29%: 100%|██████████| 13/13 [00:00<00:00, 111.97it/s]\n",
      "[4 / 50] Train: Loss = 0.19775, Accuracy = 93.74%: 100%|██████████| 572/572 [00:07<00:00, 80.87it/s]\n",
      "[4 / 50]   Val: Loss = 0.18219, Accuracy = 94.17%: 100%|██████████| 13/13 [00:00<00:00, 114.57it/s]\n",
      "[5 / 50] Train: Loss = 0.14254, Accuracy = 95.59%: 100%|██████████| 572/572 [00:07<00:00, 81.71it/s]\n",
      "[5 / 50]   Val: Loss = 0.14419, Accuracy = 95.44%: 100%|██████████| 13/13 [00:00<00:00, 114.75it/s]\n",
      "[6 / 50] Train: Loss = 0.10711, Accuracy = 96.70%: 100%|██████████| 572/572 [00:06<00:00, 89.99it/s]\n",
      "[6 / 50]   Val: Loss = 0.15228, Accuracy = 95.18%: 100%|██████████| 13/13 [00:00<00:00, 112.40it/s]\n",
      "[7 / 50] Train: Loss = 0.08766, Accuracy = 97.33%: 100%|██████████| 572/572 [00:06<00:00, 85.55it/s]\n",
      "[7 / 50]   Val: Loss = 0.12681, Accuracy = 95.91%: 100%|██████████| 13/13 [00:00<00:00, 117.53it/s]\n",
      "[8 / 50] Train: Loss = 0.07847, Accuracy = 97.62%: 100%|██████████| 572/572 [00:06<00:00, 85.44it/s]\n",
      "[8 / 50]   Val: Loss = 0.13805, Accuracy = 95.61%: 100%|██████████| 13/13 [00:00<00:00, 116.53it/s]\n",
      "[9 / 50] Train: Loss = 0.07279, Accuracy = 97.80%: 100%|██████████| 572/572 [00:07<00:00, 81.10it/s]\n",
      "[9 / 50]   Val: Loss = 0.13637, Accuracy = 95.68%: 100%|██████████| 13/13 [00:00<00:00, 111.47it/s]\n",
      "[10 / 50] Train: Loss = 0.06877, Accuracy = 97.93%: 100%|██████████| 572/572 [00:07<00:00, 81.45it/s]\n",
      "[10 / 50]   Val: Loss = 0.13984, Accuracy = 95.65%: 100%|██████████| 13/13 [00:00<00:00, 113.69it/s]\n",
      "[11 / 50] Train: Loss = 0.06664, Accuracy = 97.98%: 100%|██████████| 572/572 [00:06<00:00, 83.47it/s]\n",
      "[11 / 50]   Val: Loss = 0.10918, Accuracy = 96.58%: 100%|██████████| 13/13 [00:00<00:00, 111.96it/s]\n",
      "[12 / 50] Train: Loss = 0.06415, Accuracy = 98.06%: 100%|██████████| 572/572 [00:06<00:00, 83.31it/s]\n",
      "[12 / 50]   Val: Loss = 0.10578, Accuracy = 96.64%: 100%|██████████| 13/13 [00:00<00:00, 114.79it/s]\n",
      "[13 / 50] Train: Loss = 0.06204, Accuracy = 98.14%: 100%|██████████| 572/572 [00:07<00:00, 81.57it/s]\n",
      "[13 / 50]   Val: Loss = 0.12556, Accuracy = 96.13%: 100%|██████████| 13/13 [00:00<00:00, 110.34it/s]\n",
      "[14 / 50] Train: Loss = 0.06049, Accuracy = 98.20%: 100%|██████████| 572/572 [00:07<00:00, 80.89it/s]\n",
      "[14 / 50]   Val: Loss = 0.12137, Accuracy = 96.28%: 100%|██████████| 13/13 [00:00<00:00, 118.00it/s]\n",
      "[15 / 50] Train: Loss = 0.05841, Accuracy = 98.27%: 100%|██████████| 572/572 [00:07<00:00, 78.81it/s]\n",
      "[15 / 50]   Val: Loss = 0.10889, Accuracy = 96.57%: 100%|██████████| 13/13 [00:00<00:00, 111.47it/s]\n",
      "[16 / 50] Train: Loss = 0.05762, Accuracy = 98.31%: 100%|██████████| 572/572 [00:06<00:00, 88.74it/s]\n",
      "[16 / 50]   Val: Loss = 0.11882, Accuracy = 96.30%: 100%|██████████| 13/13 [00:00<00:00, 114.94it/s]\n",
      "[17 / 50] Train: Loss = 0.04763, Accuracy = 98.80%:   3%|▎         | 16/572 [00:00<00:06, 84.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    15: reducing learning rate of group 0 to 3.3300e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17 / 50] Train: Loss = 0.04434, Accuracy = 98.78%: 100%|██████████| 572/572 [00:07<00:00, 78.80it/s]\n",
      "[17 / 50]   Val: Loss = 0.11783, Accuracy = 96.32%: 100%|██████████| 13/13 [00:00<00:00, 113.57it/s]\n",
      "[18 / 50] Train: Loss = 0.04204, Accuracy = 98.90%: 100%|██████████| 572/572 [00:07<00:00, 80.20it/s] \n",
      "[18 / 50]   Val: Loss = 0.10859, Accuracy = 96.65%: 100%|██████████| 13/13 [00:00<00:00, 114.63it/s]\n",
      "[19 / 50] Train: Loss = 0.04225, Accuracy = 98.89%: 100%|██████████| 572/572 [00:07<00:00, 80.04it/s] \n",
      "[19 / 50]   Val: Loss = 0.11499, Accuracy = 96.43%: 100%|██████████| 13/13 [00:00<00:00, 114.81it/s]\n",
      "[20 / 50] Train: Loss = 0.04220, Accuracy = 98.90%: 100%|██████████| 572/572 [00:07<00:00, 79.45it/s] \n",
      "[20 / 50]   Val: Loss = 0.11686, Accuracy = 96.45%: 100%|██████████| 13/13 [00:00<00:00, 110.27it/s]\n",
      "[21 / 50] Train: Loss = 0.03355, Accuracy = 99.24%:   3%|▎         | 17/572 [00:00<00:06, 86.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    19: reducing learning rate of group 0 to 1.1089e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[21 / 50] Train: Loss = 0.03651, Accuracy = 99.08%: 100%|██████████| 572/572 [00:06<00:00, 86.52it/s]\n",
      "[21 / 50]   Val: Loss = 0.10738, Accuracy = 96.68%: 100%|██████████| 13/13 [00:00<00:00, 117.39it/s]\n",
      "[22 / 50] Train: Loss = 0.03628, Accuracy = 99.11%: 100%|██████████| 572/572 [00:07<00:00, 80.08it/s]\n",
      "[22 / 50]   Val: Loss = 0.11034, Accuracy = 96.62%: 100%|██████████| 13/13 [00:00<00:00, 112.05it/s]\n",
      "[23 / 50] Train: Loss = 0.03636, Accuracy = 99.12%: 100%|██████████| 572/572 [00:07<00:00, 80.29it/s]\n",
      "[23 / 50]   Val: Loss = 0.11062, Accuracy = 96.63%: 100%|██████████| 13/13 [00:00<00:00, 112.59it/s]\n",
      "[24 / 50] Train: Loss = 0.03637, Accuracy = 99.12%: 100%|██████████| 572/572 [00:06<00:00, 86.84it/s]\n",
      "[24 / 50]   Val: Loss = 0.11710, Accuracy = 96.45%: 100%|██████████| 13/13 [00:00<00:00, 115.35it/s]\n",
      "[25 / 50] Train: Loss = 0.03148, Accuracy = 99.17%:   3%|▎         | 17/572 [00:00<00:06, 91.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    23: reducing learning rate of group 0 to 3.6926e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[25 / 50] Train: Loss = 0.03381, Accuracy = 99.19%: 100%|██████████| 572/572 [00:06<00:00, 82.32it/s] \n",
      "[25 / 50]   Val: Loss = 0.11228, Accuracy = 96.53%: 100%|██████████| 13/13 [00:00<00:00, 107.70it/s]\n",
      "[26 / 50] Train: Loss = 0.03380, Accuracy = 99.20%: 100%|██████████| 572/572 [00:07<00:00, 80.08it/s]\n",
      "[26 / 50]   Val: Loss = 0.11475, Accuracy = 96.47%: 100%|██████████| 13/13 [00:00<00:00, 112.98it/s]\n",
      "[27 / 50] Train: Loss = 0.03380, Accuracy = 99.21%: 100%|██████████| 572/572 [00:06<00:00, 84.73it/s]\n",
      "[27 / 50]   Val: Loss = 0.11426, Accuracy = 96.50%: 100%|██████████| 13/13 [00:00<00:00, 109.15it/s]\n",
      "[28 / 50] Train: Loss = 0.03390, Accuracy = 99.20%: 100%|██████████| 572/572 [00:07<00:00, 79.88it/s]\n",
      "[28 / 50]   Val: Loss = 0.11368, Accuracy = 96.53%: 100%|██████████| 13/13 [00:00<00:00, 117.37it/s]\n",
      "[29 / 50] Train: Loss = 0.02137, Accuracy = 99.73%:   3%|▎         | 19/572 [00:00<00:05, 99.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    27: reducing learning rate of group 0 to 1.2296e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[29 / 50] Train: Loss = 0.03294, Accuracy = 99.24%: 100%|██████████| 572/572 [00:06<00:00, 81.93it/s]\n",
      "[29 / 50]   Val: Loss = 0.11454, Accuracy = 96.48%: 100%|██████████| 13/13 [00:00<00:00, 113.73it/s]\n",
      "[30 / 50] Train: Loss = 0.03288, Accuracy = 99.24%: 100%|██████████| 572/572 [00:06<00:00, 82.30it/s] \n",
      "[30 / 50]   Val: Loss = 0.11422, Accuracy = 96.51%: 100%|██████████| 13/13 [00:00<00:00, 109.59it/s]\n",
      "[31 / 50] Train: Loss = 0.03294, Accuracy = 99.24%: 100%|██████████| 572/572 [00:07<00:00, 79.50it/s]\n",
      "[31 / 50]   Val: Loss = 0.11489, Accuracy = 96.47%: 100%|██████████| 13/13 [00:00<00:00, 113.74it/s]\n",
      "[32 / 50] Train: Loss = 0.03293, Accuracy = 99.24%: 100%|██████████| 572/572 [00:07<00:00, 80.06it/s]\n",
      "[32 / 50]   Val: Loss = 0.11435, Accuracy = 96.51%: 100%|██████████| 13/13 [00:00<00:00, 111.74it/s]\n",
      "[33 / 50] Train: Loss = 0.04030, Accuracy = 98.83%:   3%|▎         | 16/572 [00:00<00:06, 81.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    31: reducing learning rate of group 0 to 4.0947e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[33 / 50] Train: Loss = 0.03256, Accuracy = 99.25%: 100%|██████████| 572/572 [00:06<00:00, 81.75it/s] \n",
      "[33 / 50]   Val: Loss = 0.11421, Accuracy = 96.51%: 100%|██████████| 13/13 [00:00<00:00, 114.34it/s]\n",
      "[34 / 50] Train: Loss = 0.03263, Accuracy = 99.26%: 100%|██████████| 572/572 [00:07<00:00, 80.18it/s]\n",
      "[34 / 50]   Val: Loss = 0.11410, Accuracy = 96.51%: 100%|██████████| 13/13 [00:00<00:00, 113.60it/s]\n",
      "[35 / 50] Train: Loss = 0.03265, Accuracy = 99.25%: 100%|██████████| 572/572 [00:07<00:00, 81.24it/s] \n",
      "[35 / 50]   Val: Loss = 0.11435, Accuracy = 96.49%: 100%|██████████| 13/13 [00:00<00:00, 116.40it/s]\n",
      "[36 / 50] Train: Loss = 0.03258, Accuracy = 99.25%: 100%|██████████| 572/572 [00:06<00:00, 82.31it/s] \n",
      "[36 / 50]   Val: Loss = 0.11451, Accuracy = 96.49%: 100%|██████████| 13/13 [00:00<00:00, 112.53it/s]\n",
      "[37 / 50] Train: Loss = 0.02634, Accuracy = 99.56%:   3%|▎         | 17/572 [00:00<00:06, 91.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    35: reducing learning rate of group 0 to 1.3635e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[37 / 50] Train: Loss = 0.03252, Accuracy = 99.26%: 100%|██████████| 572/572 [00:07<00:00, 81.52it/s]\n",
      "[37 / 50]   Val: Loss = 0.11494, Accuracy = 96.50%: 100%|██████████| 13/13 [00:00<00:00, 115.64it/s]\n",
      "[38 / 50] Train: Loss = 0.03247, Accuracy = 99.26%: 100%|██████████| 572/572 [00:06<00:00, 83.11it/s]\n",
      "[38 / 50]   Val: Loss = 0.11522, Accuracy = 96.49%: 100%|██████████| 13/13 [00:00<00:00, 119.03it/s]\n",
      "[39 / 50] Train: Loss = 0.03249, Accuracy = 99.26%: 100%|██████████| 572/572 [00:06<00:00, 87.85it/s] \n",
      "[39 / 50]   Val: Loss = 0.11488, Accuracy = 96.49%: 100%|██████████| 13/13 [00:00<00:00, 112.98it/s]\n",
      "[40 / 50] Train: Loss = 0.03246, Accuracy = 99.26%: 100%|██████████| 572/572 [00:06<00:00, 82.70it/s] \n",
      "[40 / 50]   Val: Loss = 0.11463, Accuracy = 96.50%: 100%|██████████| 13/13 [00:00<00:00, 111.90it/s]\n",
      "[41 / 50] Train: Loss = 0.03333, Accuracy = 99.06%:   3%|▎         | 18/572 [00:00<00:06, 91.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    39: reducing learning rate of group 0 to 4.5406e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[41 / 50] Train: Loss = 0.03243, Accuracy = 99.26%: 100%|██████████| 572/572 [00:06<00:00, 82.79it/s]\n",
      "[41 / 50]   Val: Loss = 0.11431, Accuracy = 96.50%: 100%|██████████| 13/13 [00:00<00:00, 111.80it/s]\n",
      "[42 / 50] Train: Loss = 0.03247, Accuracy = 99.26%: 100%|██████████| 572/572 [00:07<00:00, 80.81it/s]\n",
      "[42 / 50]   Val: Loss = 0.11432, Accuracy = 96.50%: 100%|██████████| 13/13 [00:00<00:00, 112.42it/s]\n",
      "[43 / 50] Train: Loss = 0.03240, Accuracy = 99.26%: 100%|██████████| 572/572 [00:06<00:00, 83.59it/s]\n",
      "[43 / 50]   Val: Loss = 0.11520, Accuracy = 96.49%: 100%|██████████| 13/13 [00:00<00:00, 111.76it/s]\n",
      "[44 / 50] Train: Loss = 0.03242, Accuracy = 99.26%: 100%|██████████| 572/572 [00:07<00:00, 80.37it/s] \n",
      "[44 / 50]   Val: Loss = 0.11500, Accuracy = 96.49%: 100%|██████████| 13/13 [00:00<00:00, 115.84it/s]\n",
      "[45 / 50] Train: Loss = 0.04392, Accuracy = 98.91%:   3%|▎         | 17/572 [00:00<00:05, 96.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    43: reducing learning rate of group 0 to 1.5120e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[45 / 50] Train: Loss = 0.03243, Accuracy = 99.26%: 100%|██████████| 572/572 [00:07<00:00, 79.81it/s]\n",
      "[45 / 50]   Val: Loss = 0.11451, Accuracy = 96.49%: 100%|██████████| 13/13 [00:00<00:00, 112.58it/s]\n",
      "[46 / 50] Train: Loss = 0.03244, Accuracy = 99.26%: 100%|██████████| 572/572 [00:06<00:00, 83.77it/s]\n",
      "[46 / 50]   Val: Loss = 0.11495, Accuracy = 96.49%: 100%|██████████| 13/13 [00:00<00:00, 112.13it/s]\n",
      "[47 / 50] Train: Loss = 0.03243, Accuracy = 99.26%: 100%|██████████| 572/572 [00:07<00:00, 76.73it/s]\n",
      "[47 / 50]   Val: Loss = 0.11448, Accuracy = 96.49%: 100%|██████████| 13/13 [00:00<00:00, 114.76it/s]\n",
      "[48 / 50] Train: Loss = 0.03243, Accuracy = 99.26%: 100%|██████████| 572/572 [00:07<00:00, 80.86it/s] \n",
      "[48 / 50]   Val: Loss = 0.11483, Accuracy = 96.49%: 100%|██████████| 13/13 [00:00<00:00, 115.54it/s]\n",
      "[49 / 50] Train: Loss = 0.03279, Accuracy = 99.12%:   3%|▎         | 17/572 [00:00<00:06, 92.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    47: reducing learning rate of group 0 to 5.0350e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[49 / 50] Train: Loss = 0.03246, Accuracy = 99.26%: 100%|██████████| 572/572 [00:06<00:00, 83.04it/s]\n",
      "[49 / 50]   Val: Loss = 0.11494, Accuracy = 96.50%: 100%|██████████| 13/13 [00:00<00:00, 110.34it/s]\n",
      "[50 / 50] Train: Loss = 0.03241, Accuracy = 99.26%: 100%|██████████| 572/572 [00:07<00:00, 81.37it/s]\n",
      "[50 / 50]   Val: Loss = 0.11454, Accuracy = 96.50%: 100%|██████████| 13/13 [00:00<00:00, 117.15it/s]\n"
     ]
    }
   ],
   "source": [
    "model_bi = BidirectionalLSTMTagger(\n",
    "    vocab_size=len(word2ind),\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
    "optimizer = optim.Adam(model_bi.parameters(), lr=1e-3, weight_decay=1.35e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.333, patience=3, verbose=True)\n",
    "\n",
    "fit(model_bi, criterion, optimizer, train_data=(X_train, y_train), \n",
    "    epochs_count=50, batch_size=64, \n",
    "    val_data=(X_val, y_val), val_batch_size=512,\n",
    "    scheduler=scheduler, scheduler_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss = 0.11307, Accuracy = 96.54%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = compute_loss_accuracy(model_bi, criterion, data=(X_test, y_test))\n",
    "print('Test: Loss = {:.5f}, Accuracy = {:.2%}'.format(test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTXmYGD_ANhm"
   },
   "source": [
    "### Предобученные эмбеддинги\n",
    "\n",
    "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
    "\n",
    "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uZpY_Q1xZ18h",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "w2v_model = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYogOoKlgtcf"
   },
   "source": [
    "Построим подматрицу для слов из нашей тренировочной выборки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VsCstxiO03oT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Know 38736 out of 45441 word embeddings\n"
     ]
    }
   ],
   "source": [
    "known_count = 0\n",
    "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
    "for word, ind in word2ind.items():\n",
    "    word = word.lower()\n",
    "    if word in w2v_model.vocab:\n",
    "        embeddings[ind] = w2v_model.get_vector(word)\n",
    "        known_count += 1\n",
    "        \n",
    "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))\n",
    "\n",
    "embeddings = torch.FloatTensor(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HcG7i-R8hbY3"
   },
   "source": [
    "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LxaRBpQd0pat"
   },
   "outputs": [],
   "source": [
    "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
    "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1, verbose=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.verbose = verbose\n",
    "        \n",
    "        word_emb_dim = w2v_model.vectors.shape[1]\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(embeddings, freeze=True)\n",
    "        self.lstm = nn.LSTM(input_size=word_emb_dim, hidden_size=lstm_hidden_dim, num_layers=lstm_layers_count, \n",
    "                            bidirectional=True)\n",
    "        self.linear = nn.Linear(in_features=2*lstm_hidden_dim, out_features=tagset_size)    \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        if self.verbose:\n",
    "            print(f'Input shape: {inputs.shape}')\n",
    "            \n",
    "        embedding_res = self.embedding(inputs)\n",
    "        if self.verbose:\n",
    "            print(f'Embedding shape: {embedding_res.shape}')\n",
    "        \n",
    "        lstm_res, _ = self.lstm(embedding_res)\n",
    "        if self.verbose:\n",
    "            print(f'LSTM output shape: {lstm_res.shape}')\n",
    "        \n",
    "        linear_res = self.linear(lstm_res)\n",
    "        if self.verbose:\n",
    "            print(f'Prediction shape: {linear_res.shape}')\n",
    "            \n",
    "        return linear_res  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 4])\n",
      "Embedding shape: torch.Size([32, 4, 100])\n",
      "LSTM output shape: torch.Size([32, 4, 128])\n",
      "Prediction shape: torch.Size([32, 4, 13])\n",
      "\n",
      "Prediction shape: torch.Size([32, 4, 13])\n",
      "True labels shape: torch.Size([32, 4])\n",
      "\n",
      "Accuracy: 0.0435\n"
     ]
    }
   ],
   "source": [
    "model_embs = LSTMTaggerWithPretrainedEmbs(\n",
    "    embeddings=embeddings,\n",
    "    tagset_size=len(tag2ind),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
    "\n",
    "logits = model_embs(X_batch)\n",
    "\n",
    "indices = torch.argmax(logits, -1)\n",
    "\n",
    "mask = y_batch.bool().float()\n",
    "match = (indices == y_batch).float()\n",
    "correct_samples = torch.sum(match * mask)\n",
    "total_samples = torch.sum(mask)\n",
    "accuracy = correct_samples/total_samples\n",
    "\n",
    "print(f'\\nPrediction shape: {logits.shape}')\n",
    "print(f'True labels shape: {y_batch.shape}')\n",
    "\n",
    "print(f'\\nAccuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1 / 50] Train: Loss = 0.46975, Accuracy = 86.27%: 100%|██████████| 572/572 [00:06<00:00, 87.74it/s]\n",
      "[1 / 50]   Val: Loss = 0.23248, Accuracy = 93.04%: 100%|██████████| 13/13 [00:00<00:00, 112.57it/s]\n",
      "[2 / 50] Train: Loss = 0.16958, Accuracy = 94.97%: 100%|██████████| 572/572 [00:06<00:00, 87.93it/s]\n",
      "[2 / 50]   Val: Loss = 0.16469, Accuracy = 94.95%: 100%|██████████| 13/13 [00:00<00:00, 113.08it/s]\n",
      "[3 / 50] Train: Loss = 0.12553, Accuracy = 96.24%: 100%|██████████| 572/572 [00:06<00:00, 86.47it/s]\n",
      "[3 / 50]   Val: Loss = 0.13700, Accuracy = 95.78%: 100%|██████████| 13/13 [00:00<00:00, 108.23it/s]\n",
      "[4 / 50] Train: Loss = 0.10457, Accuracy = 96.85%: 100%|██████████| 572/572 [00:06<00:00, 84.36it/s]\n",
      "[4 / 50]   Val: Loss = 0.12229, Accuracy = 96.14%: 100%|██████████| 13/13 [00:00<00:00, 111.21it/s]\n",
      "[5 / 50] Train: Loss = 0.09208, Accuracy = 97.21%: 100%|██████████| 572/572 [00:06<00:00, 87.88it/s]\n",
      "[5 / 50]   Val: Loss = 0.11844, Accuracy = 96.28%: 100%|██████████| 13/13 [00:00<00:00, 109.88it/s]\n",
      "[6 / 50] Train: Loss = 0.08438, Accuracy = 97.43%: 100%|██████████| 572/572 [00:06<00:00, 85.65it/s]\n",
      "[6 / 50]   Val: Loss = 0.10844, Accuracy = 96.58%: 100%|██████████| 13/13 [00:00<00:00, 110.03it/s]\n",
      "[7 / 50] Train: Loss = 0.07830, Accuracy = 97.62%: 100%|██████████| 572/572 [00:06<00:00, 87.79it/s]\n",
      "[7 / 50]   Val: Loss = 0.10658, Accuracy = 96.65%: 100%|██████████| 13/13 [00:00<00:00, 112.19it/s]\n",
      "[8 / 50] Train: Loss = 0.07394, Accuracy = 97.76%: 100%|██████████| 572/572 [00:06<00:00, 85.63it/s]\n",
      "[8 / 50]   Val: Loss = 0.10240, Accuracy = 96.76%: 100%|██████████| 13/13 [00:00<00:00, 111.84it/s]\n",
      "[9 / 50] Train: Loss = 0.07053, Accuracy = 97.84%: 100%|██████████| 572/572 [00:06<00:00, 85.56it/s]\n",
      "[9 / 50]   Val: Loss = 0.10008, Accuracy = 96.82%: 100%|██████████| 13/13 [00:00<00:00, 108.67it/s]\n",
      "[10 / 50] Train: Loss = 0.06773, Accuracy = 97.91%: 100%|██████████| 572/572 [00:06<00:00, 85.48it/s]\n",
      "[10 / 50]   Val: Loss = 0.09764, Accuracy = 96.91%: 100%|██████████| 13/13 [00:00<00:00, 109.56it/s]\n",
      "[11 / 50] Train: Loss = 0.06520, Accuracy = 98.00%: 100%|██████████| 572/572 [00:06<00:00, 90.02it/s] \n",
      "[11 / 50]   Val: Loss = 0.09737, Accuracy = 96.92%: 100%|██████████| 13/13 [00:00<00:00, 113.92it/s]\n",
      "[12 / 50] Train: Loss = 0.06315, Accuracy = 98.05%: 100%|██████████| 572/572 [00:06<00:00, 85.80it/s]\n",
      "[12 / 50]   Val: Loss = 0.09601, Accuracy = 96.96%: 100%|██████████| 13/13 [00:00<00:00, 110.68it/s]\n",
      "[13 / 50] Train: Loss = 0.06113, Accuracy = 98.11%: 100%|██████████| 572/572 [00:05<00:00, 97.40it/s] \n",
      "[13 / 50]   Val: Loss = 0.10084, Accuracy = 96.86%: 100%|██████████| 13/13 [00:00<00:00, 107.50it/s]\n",
      "[14 / 50] Train: Loss = 0.05992, Accuracy = 98.13%: 100%|██████████| 572/572 [00:06<00:00, 86.48it/s]\n",
      "[14 / 50]   Val: Loss = 0.09456, Accuracy = 96.98%: 100%|██████████| 13/13 [00:00<00:00, 105.29it/s]\n",
      "[15 / 50] Train: Loss = 0.05842, Accuracy = 98.20%: 100%|██████████| 572/572 [00:06<00:00, 94.76it/s] \n",
      "[15 / 50]   Val: Loss = 0.09289, Accuracy = 97.03%: 100%|██████████| 13/13 [00:00<00:00, 106.67it/s]\n",
      "[16 / 50] Train: Loss = 0.05727, Accuracy = 98.22%: 100%|██████████| 572/572 [00:06<00:00, 84.23it/s]\n",
      "[16 / 50]   Val: Loss = 0.09370, Accuracy = 96.98%: 100%|██████████| 13/13 [00:00<00:00, 105.44it/s]\n",
      "[17 / 50] Train: Loss = 0.05613, Accuracy = 98.26%: 100%|██████████| 572/572 [00:06<00:00, 85.15it/s]\n",
      "[17 / 50]   Val: Loss = 0.10515, Accuracy = 96.74%: 100%|██████████| 13/13 [00:00<00:00, 102.44it/s]\n",
      "[18 / 50] Train: Loss = 0.05556, Accuracy = 98.29%: 100%|██████████| 572/572 [00:06<00:00, 87.15it/s]\n",
      "[18 / 50]   Val: Loss = 0.09169, Accuracy = 97.08%: 100%|██████████| 13/13 [00:00<00:00, 112.98it/s]\n",
      "[19 / 50] Train: Loss = 0.05405, Accuracy = 98.34%: 100%|██████████| 572/572 [00:06<00:00, 84.19it/s]\n",
      "[19 / 50]   Val: Loss = 0.09147, Accuracy = 97.10%: 100%|██████████| 13/13 [00:00<00:00, 110.16it/s]\n",
      "[20 / 50] Train: Loss = 0.05330, Accuracy = 98.36%: 100%|██████████| 572/572 [00:06<00:00, 89.74it/s] \n",
      "[20 / 50]   Val: Loss = 0.09150, Accuracy = 97.08%: 100%|██████████| 13/13 [00:00<00:00, 107.49it/s]\n",
      "[21 / 50] Train: Loss = 0.05273, Accuracy = 98.37%: 100%|██████████| 572/572 [00:06<00:00, 84.69it/s]\n",
      "[21 / 50]   Val: Loss = 0.09258, Accuracy = 97.03%: 100%|██████████| 13/13 [00:00<00:00, 109.85it/s]\n",
      "[22 / 50] Train: Loss = 0.05177, Accuracy = 98.41%: 100%|██████████| 572/572 [00:06<00:00, 86.95it/s]\n",
      "[22 / 50]   Val: Loss = 0.09204, Accuracy = 97.05%: 100%|██████████| 13/13 [00:00<00:00, 113.67it/s]\n",
      "[23 / 50] Train: Loss = 0.05114, Accuracy = 98.43%: 100%|██████████| 572/572 [00:05<00:00, 96.27it/s] \n",
      "[23 / 50]   Val: Loss = 0.09412, Accuracy = 97.01%: 100%|██████████| 13/13 [00:00<00:00, 105.60it/s]\n",
      "[24 / 50] Train: Loss = 0.05260, Accuracy = 98.12%:   3%|▎         | 20/572 [00:00<00:05, 94.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    22: reducing learning rate of group 0 to 3.3300e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[24 / 50] Train: Loss = 0.04423, Accuracy = 98.70%: 100%|██████████| 572/572 [00:06<00:00, 86.88it/s] \n",
      "[24 / 50]   Val: Loss = 0.08726, Accuracy = 97.23%: 100%|██████████| 13/13 [00:00<00:00, 106.02it/s]\n",
      "[25 / 50] Train: Loss = 0.04353, Accuracy = 98.74%: 100%|██████████| 572/572 [00:06<00:00, 88.25it/s]\n",
      "[25 / 50]   Val: Loss = 0.08792, Accuracy = 97.22%: 100%|██████████| 13/13 [00:00<00:00, 111.02it/s]\n",
      "[26 / 50] Train: Loss = 0.04317, Accuracy = 98.75%: 100%|██████████| 572/572 [00:06<00:00, 85.05it/s]\n",
      "[26 / 50]   Val: Loss = 0.08789, Accuracy = 97.19%: 100%|██████████| 13/13 [00:00<00:00, 106.76it/s]\n",
      "[27 / 50] Train: Loss = 0.04304, Accuracy = 98.76%: 100%|██████████| 572/572 [00:05<00:00, 96.38it/s] \n",
      "[27 / 50]   Val: Loss = 0.08813, Accuracy = 97.21%: 100%|██████████| 13/13 [00:00<00:00, 115.16it/s]\n",
      "[28 / 50] Train: Loss = 0.04271, Accuracy = 98.76%: 100%|██████████| 572/572 [00:06<00:00, 84.77it/s]\n",
      "[28 / 50]   Val: Loss = 0.08922, Accuracy = 97.17%: 100%|██████████| 13/13 [00:00<00:00, 109.96it/s]\n",
      "[29 / 50] Train: Loss = 0.03758, Accuracy = 99.24%:   4%|▍         | 22/572 [00:00<00:04, 110.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    27: reducing learning rate of group 0 to 1.1089e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[29 / 50] Train: Loss = 0.04017, Accuracy = 98.89%: 100%|██████████| 572/572 [00:06<00:00, 85.58it/s]\n",
      "[29 / 50]   Val: Loss = 0.08772, Accuracy = 97.24%: 100%|██████████| 13/13 [00:00<00:00, 111.05it/s]\n",
      "[30 / 50] Train: Loss = 0.03984, Accuracy = 98.90%: 100%|██████████| 572/572 [00:06<00:00, 86.34it/s]\n",
      "[30 / 50]   Val: Loss = 0.08891, Accuracy = 97.23%: 100%|██████████| 13/13 [00:00<00:00, 110.05it/s]\n",
      "[31 / 50] Train: Loss = 0.03972, Accuracy = 98.90%: 100%|██████████| 572/572 [00:06<00:00, 91.42it/s] \n",
      "[31 / 50]   Val: Loss = 0.08772, Accuracy = 97.23%: 100%|██████████| 13/13 [00:00<00:00, 110.37it/s]\n",
      "[32 / 50] Train: Loss = 0.03954, Accuracy = 98.91%: 100%|██████████| 572/572 [00:06<00:00, 84.56it/s]\n",
      "[32 / 50]   Val: Loss = 0.08761, Accuracy = 97.23%: 100%|██████████| 13/13 [00:00<00:00, 109.33it/s]\n",
      "[33 / 50] Train: Loss = 0.04724, Accuracy = 98.61%:   3%|▎         | 18/572 [00:00<00:06, 89.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    31: reducing learning rate of group 0 to 3.6926e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[33 / 50] Train: Loss = 0.03865, Accuracy = 98.95%: 100%|██████████| 572/572 [00:06<00:00, 84.82it/s]\n",
      "[33 / 50]   Val: Loss = 0.08680, Accuracy = 97.24%: 100%|██████████| 13/13 [00:00<00:00, 108.80it/s]\n",
      "[34 / 50] Train: Loss = 0.03864, Accuracy = 98.95%: 100%|██████████| 572/572 [00:06<00:00, 82.86it/s]\n",
      "[34 / 50]   Val: Loss = 0.08701, Accuracy = 97.24%: 100%|██████████| 13/13 [00:00<00:00, 108.26it/s]\n",
      "[35 / 50] Train: Loss = 0.03845, Accuracy = 98.96%: 100%|██████████| 572/572 [00:06<00:00, 83.90it/s] \n",
      "[35 / 50]   Val: Loss = 0.08725, Accuracy = 97.24%: 100%|██████████| 13/13 [00:00<00:00, 108.82it/s]\n",
      "[36 / 50] Train: Loss = 0.03844, Accuracy = 98.96%: 100%|██████████| 572/572 [00:06<00:00, 83.46it/s]\n",
      "[36 / 50]   Val: Loss = 0.08696, Accuracy = 97.23%: 100%|██████████| 13/13 [00:00<00:00, 112.10it/s]\n",
      "[37 / 50] Train: Loss = 0.03846, Accuracy = 98.96%: 100%|██████████| 572/572 [00:06<00:00, 82.23it/s]\n",
      "[37 / 50]   Val: Loss = 0.08695, Accuracy = 97.23%: 100%|██████████| 13/13 [00:00<00:00, 110.15it/s]\n",
      "[38 / 50] Train: Loss = 0.04413, Accuracy = 98.80%:   3%|▎         | 18/572 [00:00<00:05, 94.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    36: reducing learning rate of group 0 to 1.2296e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[38 / 50] Train: Loss = 0.03808, Accuracy = 98.98%: 100%|██████████| 572/572 [00:06<00:00, 90.68it/s] \n",
      "[38 / 50]   Val: Loss = 0.08671, Accuracy = 97.22%: 100%|██████████| 13/13 [00:00<00:00, 105.34it/s]\n",
      "[39 / 50] Train: Loss = 0.03801, Accuracy = 98.99%: 100%|██████████| 572/572 [00:06<00:00, 93.38it/s] \n",
      "[39 / 50]   Val: Loss = 0.08690, Accuracy = 97.22%: 100%|██████████| 13/13 [00:00<00:00, 102.91it/s]\n",
      "[40 / 50] Train: Loss = 0.03799, Accuracy = 98.98%: 100%|██████████| 572/572 [00:05<00:00, 101.40it/s]\n",
      "[40 / 50]   Val: Loss = 0.08662, Accuracy = 97.22%: 100%|██████████| 13/13 [00:00<00:00, 107.66it/s]\n",
      "[41 / 50] Train: Loss = 0.03802, Accuracy = 98.98%: 100%|██████████| 572/572 [00:05<00:00, 97.78it/s] \n",
      "[41 / 50]   Val: Loss = 0.08701, Accuracy = 97.22%: 100%|██████████| 13/13 [00:00<00:00, 114.72it/s]\n",
      "[42 / 50] Train: Loss = 0.03797, Accuracy = 98.99%: 100%|██████████| 572/572 [00:06<00:00, 89.58it/s]\n",
      "[42 / 50]   Val: Loss = 0.08707, Accuracy = 97.23%: 100%|██████████| 13/13 [00:00<00:00, 109.65it/s]\n",
      "[43 / 50] Train: Loss = 0.03792, Accuracy = 98.99%: 100%|██████████| 572/572 [00:06<00:00, 89.17it/s]\n",
      "[43 / 50]   Val: Loss = 0.08689, Accuracy = 97.21%: 100%|██████████| 13/13 [00:00<00:00, 110.29it/s]\n",
      "[44 / 50] Train: Loss = 0.03791, Accuracy = 98.99%: 100%|██████████| 572/572 [00:06<00:00, 87.32it/s] \n",
      "[44 / 50]   Val: Loss = 0.08725, Accuracy = 97.22%: 100%|██████████| 13/13 [00:00<00:00, 108.45it/s]\n",
      "[45 / 50] Train: Loss = 0.03818, Accuracy = 99.20%:   3%|▎         | 18/572 [00:00<00:05, 95.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    43: reducing learning rate of group 0 to 4.0947e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[45 / 50] Train: Loss = 0.03777, Accuracy = 98.99%: 100%|██████████| 572/572 [00:06<00:00, 85.89it/s]\n",
      "[45 / 50]   Val: Loss = 0.08696, Accuracy = 97.21%: 100%|██████████| 13/13 [00:00<00:00, 110.39it/s]\n",
      "[46 / 50] Train: Loss = 0.03778, Accuracy = 98.99%: 100%|██████████| 572/572 [00:05<00:00, 100.32it/s]\n",
      "[46 / 50]   Val: Loss = 0.08680, Accuracy = 97.22%: 100%|██████████| 13/13 [00:00<00:00, 112.00it/s]\n",
      "[47 / 50] Train: Loss = 0.03778, Accuracy = 98.99%: 100%|██████████| 572/572 [00:06<00:00, 86.46it/s]\n",
      "[47 / 50]   Val: Loss = 0.08750, Accuracy = 97.22%: 100%|██████████| 13/13 [00:00<00:00, 112.06it/s]\n",
      "[48 / 50] Train: Loss = 0.03777, Accuracy = 98.99%: 100%|██████████| 572/572 [00:06<00:00, 89.91it/s] \n",
      "[48 / 50]   Val: Loss = 0.08710, Accuracy = 97.22%: 100%|██████████| 13/13 [00:00<00:00, 111.53it/s]\n",
      "[49 / 50] Train: Loss = 0.03991, Accuracy = 99.15%:   3%|▎         | 19/572 [00:00<00:05, 99.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    47: reducing learning rate of group 0 to 1.3635e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[49 / 50] Train: Loss = 0.03772, Accuracy = 99.00%: 100%|██████████| 572/572 [00:06<00:00, 86.05it/s] \n",
      "[49 / 50]   Val: Loss = 0.08699, Accuracy = 97.22%: 100%|██████████| 13/13 [00:00<00:00, 105.95it/s]\n",
      "[50 / 50] Train: Loss = 0.03769, Accuracy = 99.00%: 100%|██████████| 572/572 [00:06<00:00, 84.10it/s]\n",
      "[50 / 50]   Val: Loss = 0.08738, Accuracy = 97.22%: 100%|██████████| 13/13 [00:00<00:00, 108.70it/s]\n"
     ]
    }
   ],
   "source": [
    "model_embs = LSTMTaggerWithPretrainedEmbs(\n",
    "    lstm_hidden_dim=128,\n",
    "    embeddings=embeddings,\n",
    "    tagset_size=len(tag2ind)\n",
    ").cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
    "optimizer = optim.Adam(model_embs.parameters(), lr=1e-3, weight_decay=0.5e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.333, patience=3, verbose=True); scheduler_loss=True\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.333); scheduler_loss=False\n",
    "\n",
    "fit(model_embs, criterion, optimizer, train_data=(X_train, y_train), \n",
    "    epochs_count=50, batch_size=64, \n",
    "    val_data=(X_val, y_val), val_batch_size=512,\n",
    "    scheduler=scheduler, scheduler_loss=scheduler_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Ne_8f24h8kg"
   },
   "source": [
    "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
    "\n",
    "Добейтесь качества лучше прошлых моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HPUuAPGhEGVR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss = 0.08742, Accuracy = 97.25%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = compute_loss_accuracy(model_embs, criterion, data=(X_test, y_test))\n",
    "print('Test: Loss = {:.5f}, Accuracy = {:.2%}'.format(test_loss, test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Week 06 - RNNs, part 2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
